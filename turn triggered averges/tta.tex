% -*- TeX -*- -*- UK -*-
% ----------------------------------------------------------------
% arXiv Paper ************************************************
%
% Subhaneil Lahiri's template
%
% Before submitting:
%    Comment out hyperref
%    Comment out showkeys
%    Replace \input{mydefs.tex} with its contents
%       or include mydefs.tex in zip/tar file
%    Replace \input{newsymb.tex} with its contents
%       or include newsymb.tex in zip/tar file
%    Put this file, the .bbl file, any picture or
%       other additional files and natbib.sty
%       file in a zip/tar file
%
% **** -----------------------------------------------------------
\documentclass[12pt]{article}
% Preamble:
\usepackage{a4wide}
\usepackage[centertags]{amsmath}
\usepackage{amssymb}
\usepackage[sort&compress,numbers]{natbib}
%\usepackage{citeB}
\usepackage{ifpdf}
\usepackage{graphicx}
%\usepackage{graphics} for finding documentation only
%\usepackage{xcolor}
%\usepackage{pgf}
\ifpdf
\usepackage[pdftex,bookmarks]{hyperref}
\else
\usepackage[hypertex]{hyperref}
\DeclareGraphicsRule{.png}{eps}{.bb}{}
\fi
%
% >> Only for drafts! <<
\usepackage[notref,notcite]{showkeys}
% ----------------------------------------------------------------
\vfuzz2pt % Don't report over-full v-boxes if over-edge is small
\hfuzz2pt % Don't report over-full h-boxes if over-edge is small
%\numberwithin{equation}{section}
%\renewcommand{\baselinestretch}{1.5}
% ----------------------------------------------------------------
\usepackage{amsthm}
\newtheoremstyle{sldefinition}%
  {3pt}%space above
  {3pt}%space below
  {}%body font
  {}%indent amount
  {\bfseries}%theorem head font
  {}%theorem head punctuation
  {\newline}%space after head
  {\thmname{#1}\thmnumber{ #2}:{\bfseries\thmnote{ #3}}}%head spec
\newtheoremstyle{slplain}%
  {3pt}%space above
  {3pt}%space below
  {}%body font
  {}%indent amount
  {\bfseries}%theorem head font
  {}%theorem head punctuation
  {\newline}%space after head
  {\thmname{#1}\thmnumber{ #2}{\bfseries\thmnote{ (#3)}}:}%head spec
%
\theoremstyle{slplain}
\newtheorem{thm}{Theorem}
%
\theoremstyle{sldefinition}
\newtheorem{defn}{Definition}
%
\theoremstyle{remark}
\newtheorem*{notn}{Notation}
\newtheorem*{rem}{Remark}
% ----------------------------------------------------------------
% New commands etc.
\input{mydefs.tex}
\input{newsymb.tex}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title info:
\title{Non-linear, history-dependent response functions}
%
% Author List:
%
\author{Subhaneil Lahiri
\\
%
% Addresses:
%
\small{\emph{Harvard University}}
%
}

\begin{document}

\maketitle

%% Preprint numbers, etc.
%\preprintno{8cm}{6cm}{
%    \texttt{arXiv:yymm.nnnn [hep-th]}
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{abstract}
  We look at methods of characterising non-linear, history-dependent responses, particularly the rates of inhomogeneous Poisson processes.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beginning of Article:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}\label{sec:intro}

In this note we will look at responses, $r(t)$, that are \emph{functionals} of some stimulus, $s(t)$,
%
\begin{equation}\label{eq:response}
  r(t) = r_t[s].
\end{equation}
%

We will look at types of series expansion: the Volterra series in \sref{sec:volterra} that is analogous to the Taylor expansion for function; and the Wiener series in \sref{sec:wiener} that is analogous to the expansion in Hermite polynomials for functions.

When constructing a set of orthogonal polynomials, one has to choose a weighting function for the integration measure:
%
\begin{equation}\label{eq:functionprod}
  f \cdot g = \int f(t) g(t)\, w(t) \dr t.
\end{equation}
%
For Hermite polynomials, one chooses $w(t)=\e^{-t^2}$, for Legendre polynomials, one chooses $w(t)=\theta(1-t)\theta(1+t)$ and for Laguerre polynomials, one chooses $w(t)=\theta(t)\e^{-t}$.

For orthogonal functionals, one has to choose a measure for the space of functions:
%
\begin{equation}\label{eq:functionalprod}
  F \cdot G = \int F[s] G[s]\, W[s] \CD s.
\end{equation}
%
In analogy with the Hermite polynomials, we will choose
%
\begin{equation}\label{eq:whitenoisemeasure}
  W[s] \propto \exp\prn{-\int \frac{s(t)^2}{2\sigma^2}\,\dr t}.
\end{equation}
%

There is a statistical way of looking at this that is helpful when thinking about how to measure these quantities. In the case of functions, one can normalise $w(t)$ and think of it as a probability density, then we have $f \cdot g = \av{f(t)g(t)}$, where $t$ is a random variable whose probability distribution is given by $w(t)$.

For functionals, we should think of the input, $s(t)$, as being a \emph{stochastic process}. The stochastic process corresponding to \eqref{eq:whitenoisemeasure} is called zero-mean, Gaussian white noise. We will discuss this in more detail in \sref{sec:whitenoise}.

When the response function, $r(t)$, is the rate of an inhomogeneous Poisson process, it can be difficult to measure. One has to repeat the stimulus many times and count the events in various time bins. If one wishes to use small time bins, one has to repeat the stimulus more times to get enough data in each bin.

Instead, there is a simpler method where one compute averages of the \emph{stimulus} at various time shifts before each event. With this method, one never needs to determine $r(t)$. We will discuss this further in \sref{sec:poisson}.


\section{Volterrra series}\label{sec:volterra}

Suppose we have a functional that is also a function of the independent variable, \eg if we have a system that responds to some time varying input, $x(t)$, in a history dependent way (\ie the system has some memory), then the response at time $t$ is a function of $t$ as well as a functional of $x(t')$.
The Volterra series is a way of expanding such a functional in powers of the input. It is analogous to the Taylor series expansion of a function.

\begin{defn}[Volterra functional]
  A Volterra functional of degree $p$ is one of the form:
  %
  \begin{equation}\label{eq:volfunc}
    V\rp{p}_t[x] = \prod_{i=1}^p \int\!\dr t_i\,  v(t_1,\ldots,t_p) \prod_{i=1}^p x(t-t_i).
  \end{equation}
  %
  Without loss of generality, we may assume that the function $v(t_1,\ldots,t_p)$ is symmetric under interchange of any of its arguments. It is causal if $v$ vanishes whenever any of its arguments is negative.
\end{defn}


\begin{defn}[Volterra series]
  A Volterra series of degree $n$ is a sum of Volterra functionals of degree less than or equal to $n$:
  %
  \begin{equation}\label{eq:volser}
    F_t[x] = \sum_{p=0}^n\prod_{i=1}^p \int\!\dr t_i\,  f_p(t_1,\ldots,t_p) \prod_{i=1}^p x(t-t_i).
  \end{equation}
  %
  The term ``Voltera series'', without specification of degree, shall be taken to mean a Volterra series of infinite degree.
\end{defn}

\begin{defn}[Volterra kernel]
  The $p^{\text{th}}$ Volterra kernel is the function $f_p(t_1,\ldots,t_p)$ that appears in \eqref{eq:volser}.
\end{defn}

\begin{thm}[Determination of Volterra kernels]
  Suppose we are given some functional, $F_t[x]$, and we want to express it as a Volterra sreies. The Volterra kernels that we would use are given by
  %
  \begin{equation}\label{eq:volkern}
    f_p(t_1,\ldots,t_p) = \frac{1}{p!}\, \frac{\delta^p F_t[0]}{\delta x(t-t_1) \ldots \delta x(t-t_p)}.
  \end{equation}
  %
\end{thm}
\begin{proof}
  Substitute \eqref{eq:volser} into \eqref{eq:volkern} and see what you get.
\end{proof}

The Volterra expansion is only valid for inputs close to zero if all the functional derivatives in \eqref{eq:volkern} exist, similar to how a Taylor series can only be used for analytic functions. In addition, the Taylor series is only valid inside the radius of convergence, which is equal to the distance to the nearest non-analyticity. This is one of the reasons why orthogonal function expansions are often better than Taylor series: in return for demanding a weaker form of convergence, the class of functions that can be represented is larger.


\begin{rem}[impulse response]
  The experimental analogue of \eqref{eq:volkern} involves measuring the response to a series of impulses. Let
  %
  \begin{equation}\label{eq:imulses}
  \begin{aligned}
    x(t) &= \sum_{i=1}^p \epsilon_i \delta(t-t_i), \\
    F(t;\vec{t},\vec{\epsilon} ) &= F_t[x],
  \end{aligned}
  \end{equation}
  %
  where $\vec{t}$ represents $(t_1,\ldots,t_p)$, etc. Then one can determine the Volterra kernels via
  %
  \begin{equation}\label{eq:impkern}
    f_p(t-t_1,\ldots,t-t_p) = \frac{1}{p!}\, \frac{\p^p F(t;\vec{t},\vec{0})}{\p\epsilon_1\ldots\p\epsilon_p}.
  \end{equation}
  %
  However, one has to perform this experiment several times, varying $\vec{\epsilon}$ and $\vec{t}$, in order to determine one of the Volterra kernels.
\end{rem}


\section{Gaussian white noise}\label{sec:whitenoise}

\begin{defn}[Stationary process]
  A stochastic process, $x(t)$ is said to be stationary if
  %
  \begin{equation}\label{eq:statproc}
    \av{x(t_1)x(t_2) \ldots x(t_n)} = \av{x(t_1+\tau)x(t_2+\tau) \ldots x(t_n+\tau)}
    \qquad \forall n,t_i,\tau
  \end{equation}
  %
\end{defn}

\begin{defn}[Gaussian process]
  A stochastic process is said to be Gaussian if
  %
  \begin{equation}\label{eq:gaussproc}
    \av{\e^{\ir\!\intd{t} [J(t) x(t)]}} = \e^{\ir\!\intd{t} [J(t)\mu(t)] - \frac{1}{2}\intd{t\dr t'} [J(t)J(t')\phi(t,t')]}.
  \end{equation}
  %
  This means that all correlation functions can be expressed in terms of the connected one and two-point functions, \eg
  %
  \begin{equation}\label{eq:gaussmom}
    \begin{aligned}
      \av{x(t_1)} &= \mu(t_1), \\
      \av{x(t_1)x(t_2)}  &= \mu(t_1)\mu(t_2) + \phi(t_1,t_2), \\
      \av{x(t_1)x(t_2)x(t_3)}  &= \mu(t_1)\mu(t_2)\mu(t_3) + \phi(t_1,t_2)\mu(t_3) + \phi(t_1,t_3)\mu(t_2) + \phi(t_2,t_3)\mu(t_1).
    \end{aligned}
  \end{equation}
  %
  If a process is Gaussian and stationary,
  %
  \begin{equation}\label{eq:statgauss}
    \begin{aligned}
      \mu(t) &= \mu, \\
      \phi(t,t')  &= \phi(t-t').
    \end{aligned}
  \end{equation}
  %
  The Fourier transform of $\phi(t)$, $\tilde{\phi}(\omega)$, is called the \textbf{power density spectrum} of $x(t)$.
\end{defn}

\begin{defn}[Gaussian white noise]
  Gaussian white noise of power $\sigma^2$ is a stationary Gaussian process with
  %
  \begin{equation}\label{eq:gaussianwhite}
    \phi(t-t') = \sigma^2\delta(t-t'), \qquad
    \tilde{\phi}(\omega) = \sigma^2.
  \end{equation}
  %
  \ie the power density spectrum is the same for all frequencies. In practice, one can only have white noise up to some high frequency cut-off.
\end{defn}

Most of the time, we will be dealing with zero mean gaussian white noise, \ie
%
\begin{equation}\label{eq:zeromeanwhite}
  \mu=0, \qquad
  \av{\e^{\ir\!\intd{t} [J(t) x(t)]}} = \e^{-\frac{\sigma^2}{2}\!\intd{t} [J(t)]^2}.
\end{equation}
%


\section{Wiener series}\label{sec:wiener}

The Wiener series is an alternative series expansion of a response functional (the type described at the beginning of \sref{sec:volterra}) that is analogous to an expansion of a function as a series of orthogonal functions. For this, we need to define an inner product for functionals:
%
\begin{equation}\label{eq:innerfunc}
  F \cdot G = \av{F_t[x]G_t[x]} = \int\!\CD x \brc{F_t[x]G_t[x]\,\e^{-\intd{t}\brk{\frac{x(t)^2}{2\sigma^2}}}},
\end{equation}
%
\ie $x(t)$ is zero mean Gaussian white noise with power $\sigma^2$.

\begin{defn}[Wiener functional]
  A Wiener functional of degree $p$ is a Volterra series of degree $p$ that is orthogonal to all Volterra functionals of degree less than $p$.
\end{defn}

Once the $p^{\text{th}}$ Volterra kernel of a degree $p$ Wiener functional has been specified, all the lower order Volterra kernels are determined. We will use the notation $G\rp{p}_t[k;x]$ for a degree $p$ Wiener functional whose $p^{\text{th}}$ Volterra kernel is $k$, at time $t$ with input $x$. \eg \cite[ch.12]{schetzen:1980}:
%
\begin{equation}\label{eq:wienerex}
  \begin{aligned}
    G\rp{0}_t[k;x] =& k, \\
    G\rp{1}_t[k;x] =& \intd{t_1}k(t_1)x(t-t_1), \\
    G\rp{2}_t[k;x] =& \intd{t_1\dr t_2}k(t_1,t_2)x(t-t_1)x(t-t_2)
                     -\sigma^2\intd{t_1}k(t_1,t_1), \\
    G\rp{3}_t[k;x] =& \intd{t_1\dr t_2\dr t_3}k(t_1,t_2,t_3)x(t-t_1)x(t-t_2)x(t-t_3)\\
                     &-3\sigma^2\intd{t_1\dr t_2}k(t_1,t_2,t_2))x(t-t_1). 
  \end{aligned}
\end{equation}
%
Note that the Wiener functionals also depend on the choice of $\sigma^2$.

\begin{defn}[Wiener series]
  A Wiener series of degree $n$ is a sum of Wiener functionals of degree less than or equal to $n$:
  %
  \begin{equation}\label{eq:wienerser}
    F_t[x] = \sum_{p=0}^n G\rp{p}_t[k_p;x].
  \end{equation}
  %
  The term ``Wiener series'', without specification of degree, shall be taken to mean a Wiener series of infinite degree.
\end{defn}

\begin{defn}[Wiener kernel]
  The $p^{\text{th}}$ Wiener kernel is the function $k_p(t_1,\ldots,t_p)$ that appears in \eqref{eq:wienerser}.
\end{defn}

\begin{thm}[Inner product of Wiener functionals]
  The inner product of two Wiener functionals of arbitrary degree with kernels $f$ and $g$ is:
  %
  \begin{equation}\label{eq:wienerinner}
    \av{G\rp{p}_t[f;x] G\rp{q}_t[g;x]} = \delta_{p,q}\, p! \sigma^{2p} \intd{t_1\ldots \dr t_p} f(t_1,\ldots,t_p) g(t_1,\ldots,t_p).
  \end{equation}
  %
\end{thm}
\begin{proof}
  see \cite[\S14.2]{schetzen:1980}
\end{proof}

A particularly useful set of Wiener functionals are:
%
\begin{equation}\label{eq:wienerdelta}
  \begin{aligned}
    D\rp{p}_{t,\vec{\tau}}[x] =& \, G\rp{p}_t[\delta\rp{p}_{\vec{\tau}};x], \\ \text{where}\quad
    \delta\rp{p}_{\vec{\tau}}(t_1,\ldots,t_p) =& \, \frac{1}{p!}\,\sum_{\pi\in S_p} \prod_{i=1}^p \delta(t_i-\tau_{\pi(i)}),
  \end{aligned}
\end{equation}
%
where $S_p$ is the group of permutations of $p$ objects. 
\eg
%
\begin{equation}\label{eq:wienerdeltaex}
  \begin{aligned}
    D\rp{0}_{t,\vec{tau}} =&\, 1, \\
    D\rp{1}_{t,\vec{tau}} =&\, x(t-\tau_1), \\
    D\rp{2}_{t,\vec{tau}} =&\, x(t-\tau_1)x(t-\tau_2)-\sigma^2\delta(\tau_1-\tau_2), \\
    D\rp{3}_{t,\vec{tau}} =&\, x(t-\tau_1)x(t-\tau_2)x(t-\tau_3)\\& - \sigma^2 [\delta(\tau_1-\tau_2)x(t-\tau_3) + \delta(\tau_1-\tau_3)x(t-\tau_2) + \delta(\tau_2-\tau_3)x(t-\tau_1) ]. \\
  \end{aligned}
\end{equation}
%
In general:
%
\begin{equation}\label{eq:wienerdeltagen}
  D\rp{p}_{t,\vec{\tau}}[x] = \exp\prn{-\frac{\sigma^2}{2}\intd{\tau}\fdf{^2}{x(\tau)^2}} \prod_{i=1}^p x(t-\tau_i).
\end{equation}
%
\begin{proof}
  It's not difficult to see that the highest degree terms in \eqref{eq:wienerdelta} and \eqref{eq:wienerdeltagen} agree. Therefore we only need to show that it is orthogonal to all lower degree Volterra functionals. In fact, it suffices to show that it is orthogonal to $\prod_{i=1}^q x(t-s_i)$ for $q<p$, as these form a basis for the Volterra functionals.
  %
  \begin{equation}\label{eq:wienerdeltavol}
    \av{\brk{\e^{-\frac{\sigma^2}{2}\intd{\tau}\fdf{^2}{x(\tau)^2}} \prod_{i=1}^p x(t-\tau_i)}\prod_{j=1}^q x(t-s_i)} = 0 \quad \forall q<p.
  \end{equation}
  %
  We can construct a generating function for the left hand side:
  %
  \begin{equation}\label{eq:wienerdeltavolgen}
    \begin{aligned}
      Z[J,J'] &= \brk{\e^{-\frac{\sigma^2}{2}\intd{\tau}\fdf{^2}{x(\tau)^2}} \e^{\ir\!\intd{t'} [J(t') x(t-t')]}} \e^{\ir\!\intd{s} [J'(s) x(t-s)]},\\
      \frac{(-\ir)^{p+q}\delta^{p+q}Z[0,0]}{\delta J(\tau_1)\ldots\delta J(\tau_p) \delta J'(s_1)\ldots\delta J'(s_q)}  &= \brk{\e^{-\frac{\sigma^2}{2}\intd{\tau}\fdf{^2}{x(\tau)^2}} \prod_{i=1}^p x(t-\tau_i)}\prod_{j=1}^q x(t-s_i).
    \end{aligned}
  \end{equation}
  %
  Using \eqref{eq:zeromeanwhite},
  %
  \begin{equation*}
    \begin{aligned}
      Z[J,J'] &= \e^{-\frac{\sigma^2}{2}\!\intd{t} [J(t)]^2} \e^{\ir\!\intd{s} [J(t-s)+J'[t-s] x(s)}, \\
       \av{Z[J,J']} &= \e^{-\frac{\sigma^2}{2}\intd{s} J'(s)[J'(s)+2J(s)]},\\
       \frac{(-\ir)^{p}\delta^{p}\av{Z[0,0]}}{\delta J(\tau_1)\ldots\delta J(\tau_p)}  &= (\ir\sigma^2)^p \prn{\prod_{i=1}^p J'(\tau_i)}\e^{-\frac{\sigma^2}{2}\intd{s} J'(s)^2}.
    \end{aligned}
  \end{equation*}
  %
  Therefore, if we take the functional derivative with respect to $J'(s_j)$ $q$ times, with $q<p$, we will still have some of the factors of $J'(\tau_i)$ left in front. When we set $J'(s)=0$ after that, we will get zero, \ie
  %
  \begin{equation*}
  \begin{aligned}
    \frac{(-\ir)^{p+q}\delta^{p+q}\av{Z[0,0]}}{\delta J(\tau_1)\ldots\delta J(\tau_p) \delta J'(s_1)\ldots\delta J'(s_q)}  &= \av{\brk{\e^{-\frac{\sigma^2}{2}\intd{\tau}\fdf{^2}{x(\tau)^2}} \prod_{i=1}^p x(t-\tau_i)}\prod_{j=1}^q x(t-s_i)}\\ &= 0 \quad \forall q<p.
  \end{aligned}
  \end{equation*}
  %
\end{proof}

Using \eqref{eq:wienerinner}, we have
%
\begin{equation}\label{eq:wienerkern}
  \av{F_t[x] D\rp{p}_{t,\vec{\tau}}[x]} = p!\sigma^{2p}k_p(\tau_1,\ldots,\tau_p),
\end{equation}
%
where $k_p$ is the $p^{\text{th}}$ Wiener kernel of $F_t$. This formula can be used to determine the kernels to use if we want to represent a functional as a Wiener series. One can show that this choice is optimal, in a least squares sense. Note that the kernels depend on the choice of $\sigma^2$, which should not be a surprise, as the Wiener functionals themselves depend on it.




\section{Inhomogeneous Poisson processes}\label{sec:poisson}





%\section*{Acknowledgements}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section*{Appendices}
%\appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{utcaps_sl}
\bibliography{maths}

\end{document}
