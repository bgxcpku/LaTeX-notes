% -*- TeX -*- -*- UK -*-
% ----------------------------------------------------------------
% arXiv Paper ************************************************
%
% Subhaneil Lahiri's template
%
% Before submitting:
%    Comment out hyperref
%    Comment out showkeys
%    Replace \input{mydefs.tex} with its contents
%       or include mydefs.tex in zip/tar file
%    Replace \input{newsymb.tex} with its contents
%       or include newsymb.tex in zip/tar file
%    Put this file, the .bbl file, any picture or
%       other additional files and natbib.sty
%       file in a zip/tar file
%
% **** -----------------------------------------------------------
\documentclass[12pt]{article}
% Preamble:
\usepackage{a4wide}
\usepackage[centertags]{amsmath}
\usepackage{amssymb}
%\usepackage{amsthm}
\usepackage[sort&compress,numbers]{natbib}
%\usepackage{citeB}
\usepackage{ifpdf}
\usepackage{graphicx}
%\usepackage{graphics} for finding documentation only
%\usepackage{xcolor}
%\usepackage{pgf}
\ifpdf
\usepackage[pdftex,bookmarks]{hyperref}
\else
\usepackage[hypertex]{hyperref}
\DeclareGraphicsRule{.png}{eps}{.bb}{}
\fi
\usepackage{epstopdf}
\epstopdfsetup{update,suffix=-generated}
%
% >> Only for drafts! <<
\usepackage[notref,notcite]{showkeys}
% ----------------------------------------------------------------
\vfuzz2pt % Don't report over-full v-boxes if over-edge is small
\hfuzz2pt % Don't report over-full h-boxes if over-edge is small
%\numberwithin{equation}{section}
%\renewcommand{\baselinestretch}{1.5}
% ----------------------------------------------------------------
% New commands etc.
\input{mydefs.tex}
\input{newsymb.tex}
\input{units.tex}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title info:
\title{Mutual information between successive reorientations}
%
% Author List:
%
\author{Subhaneil Lahiri
\\
%
% Addresses:
%
\small{\emph{Harvard University}}
%
}

\begin{document}

\maketitle



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{abstract}
  We show how mutual information can be used to describe the independence of successive reorientations
\end{abstract}

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beginning of Article:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Reorientation sequences}\label{sec:reoseq}

As a worm navigates, it performs a sequence of turns. When turns occur sufficiently close to each other, they are grouped into a reorientation event. These reorientations have several characteristics, \eg the types of turn of which it is composed, the difference in heading direction before and after, the duration of the run leading into it. We wish to know if the characteristics of one reorientation are independent of the characteristics of previous reorientations.

Consider a sequence of $r$ successive reorientations. The values of a particular characteristic of these reorientations is an $r$-tuple of random variables\footnote{ Actually, reorientation type is not a random variable, as it is not described by a number. However, the concept of a probability distribution still makes sense, and the probability of the outcome can be considered a random variable. }: $(X_1,\ldots,X_r)$. We are asking whether or not $P(X_1,\ldots,X_r) = P(X_1)\ldots P(X_r)$. We will discuss some measures of independence in \sref{sec:entropy}.

These probability distributions can be estimated from the frequencies in a sample sequence (described schematically in \fref{fig:schematic}). However, as a consequence of finite sample size, this process has both systematic and random errors. We will look at several methods for removing systematic errors in \sref{sec:syscorr} and one method for estimating random errors in appendix \ref{sec:stderr}.

\begin{figure}
  \begin{center}
    \includegraphics[width=10cm]{schematic.eps}
  \end{center}
  %\input{.TpX}
  \caption{Schematic calculation of probabilities. (a) Sample sequence of reorientations. (b) Grouping sequence into $r$-tuples ($r=2$). (c) Estimating probabilities for individual members and for $r$-tuples from relative frequencies in sample.} \label{fig:schematic}
\end{figure}

In principle we should look at all the characteristics together. However, increasing the dimensionality of the data at each reorientation increases the number of bins used, which in turn increases the number of samples needed. Therefore, we will have to be satisfied with looking at each characteristic separately.

Note that this method can confuse animal-to-animal variability with causation. \Eg suppose that one of the worms makes larger turns than the others. In this situation, if the first reorientation in an $r$-tuple is large it would make it more likely that the worm in question is the large-turning one, making it more likely that the other reorientations in the $r$-tuple are large. Thus the probability distributions would not show independence even in the absence of actual causation. The same thing applies to lack of stationarity. If runs tend to get longer over time, the same issue will arise.


\section{Entropy and mutual information}\label{sec:entropy}

The \textbf{entropy} of a probability distribution is a measure of the lack of information we have about a random variable \cite{Cover:2006}:
%
\begin{equation}\label{eq:ent}
  H(X) = \av{-\log P(X)}.
\end{equation}
%
It takes its minimum value of $0$ when $X$ can only take one value. It takes its maximum value of $\log n$ when $X$ has a uniform distribution over $n$ possibilities.

With several random variables, we can define a joint entropy from their joint probability distribution:
%
\begin{equation}\label{eq:jointent}
  H(X_1,\ldots,X_r) = \av{-\log P(X_1,\ldots,X_r)}.
\end{equation}
%
It satisfies the bounds
%
\begin{equation}\label{eq:entbounds}
  \max_i H(X_i) \leq H(X_1,\ldots,X_r) \leq \sum_{i=1}^r H(X_i).
\end{equation}
%
The lower bound is saturated when one of the variables is enough to determine the others. The upper bound is saturated when the $X_i$ are independent:
%
\begin{equation}\label{eq:indent}
  P(X_1,\ldots,X_r) = \prod_{i=1}^r P(X_i)
  \quad \implies \quad
  H(X_1,\ldots,X_r) = \sum_{i=1}^r H(X_i).
\end{equation}
%

We can define the following measure of (lack of) independence:
%
\begin{equation}\label{eq:mutinf}
  I(X_1,\ldots,X_r) = \sum_{i=1}^r H(X_i) - H(X_1,\ldots,X_r) = \av{ \log \frac{P(X_1,\ldots,X_r)}{P(X_1)\ldots P(X_r)} }.
\end{equation}
%
In the case $r=2$, this is the \textbf{mutual information} between $X_1$ and $X_2$. For $r>2$, there are many different generalisations of mutual information. This one is called the \textbf{total correlation} \cite{Watanabe:1960}, or multiinformation. It has the properties:
%
\begin{itemize}
  \item it vanishes if and only if the random variables are independent
  \item otherwise, it is positive.
  \item it is bounded from above by $\sum_{i=1}^r H(X_i) - \max_i H(X_i)$.
\end{itemize}
%
We can define a normalised total correlation:
%
\begin{equation}\label{eq:normmutinfgen}
  C_r = \frac{I_r}{\sum_{i=1}^r H(X_i) - \max_i H(X_i)}, \qquad 0 \leq C_r \leq 1.
\end{equation}
%
The lower bound corresponds to complete independence. The upper bound corresponds to complete redundancy.

In our cases, the random variables, $X_i$, all have the same marginal distribution, so $H(X_i)=H_1\; \forall i$ and the total correlation satisfies the bounds
%
\begin{equation}\label{eq:mutinfbounds}
  0 \leq I_r(X_1,\ldots,X_r) \leq (r-1)H_1.
\end{equation}
%
and the normalised version is:
%
\begin{equation}\label{eq:normmutinf}
  C_r = \frac{I_r}{(r-1)H_1}, \qquad 0 \leq C_r \leq 1.
\end{equation}
%
We will use this version except where explicitly stated otherwise.

\subsection{Entropy, information and bits}\label{sec:bits}

If we take the logarithms to base 2, we can think of entropy as a measure of ``the average number of bits'' of information we are missing by not knowing the value of $X$, and hence the number of bits of information we'd gain by measuring it. \Eg suppose $X$ can take on two values with equal probability, then $H(X)=1$. If the probabilities were 1 and 0, then $H(X)=$, as we'd gain no information by measuring it due to the fact that we already know its value. If we have $n$ independent two state systems of the first type, we'd have an entropy of $n$ due to \eqref{eq:indent}. On the other hand, if the first bit determined the remaining $n-1$ bits, we'd have an entropy of 1.

There is another way of thinking of mutual information in terms of \textbf{conditional entropy}:
%
\begin{equation}\label{eq:condent}
  H(Y|X) = \av{-\log P(Y|X)},
\end{equation}
%
where the average is over all values of $X$ and $Y$. This can be thought of as the number of bits of missing information due to not knowing $Y$ that remain after measuring $X$.

Then the mutual information can be written as
%
\begin{equation}\label{eq:mutinfcond}
  I(X,Y) = H(Y) - H(Y|X) = H(X) - H(X|Y),
\end{equation}
%
\ie it is the number of bits of information about $Y$ that we gain by measuring $X$ (and vice-versa).

When we have three random variables, we can write
%
\begin{equation}\label{eq:multiinfcond3}
  \begin{aligned}
    I_3 &= \av{\log \frac{P(X_1,X_2,X_3)}{P(X_1)P(X_2)P(X_3)}} \\
      &=  \av{\log \frac{P(X_2 \vert X_1)}{P(X_2)}} + \av{\log \frac{P(X_3 \vert X_1,X_2)}{P(X_3)}} \\
      &= \brk{H(X_2)-H(X_2|X_1)} +  \brk{ H(X_3)-H(X_3|X_1,X_2)}.
  \end{aligned}
\end{equation}
%
In general, we can write
%
\begin{equation}\label{eq:multiinfcondgen}
  \begin{aligned}
    I_r &= \av{\log \frac{P(X_1,X_2,\ldots,X_r)}{P(X_1)P(X_2)\ldots P(X_r)}} \\
      &=  \av{\log \frac{P(X_2 \vert X_1)}{P(X_2)}} +\ldots+ \av{\log \frac{P(X_r \vert X_1,\ldots,X_{r-1})}{P(X_r)}} \\
      &= \brk{H(X_2)-H(X_2|X_1)} + \ldots + \brk{ H(X_r)-H(X_r|X_1,\ldots,X_{r-1})} \\
      &= \sum_{i=2}^{r} \brk{ H(X_i)-H(X_i|X_1,\ldots,X_{i-1})}.
  \end{aligned}
\end{equation}
%
\ie its the sum of number of bits of information about each $X_i$ that we gain by measuring the previous ones.


\subsection{Markov processes}\label{sec:markov}

If the process is Markovian, \ie one reorientation/run is only affected by the previous reorientation/run and not any earlier ones, then $P(X_3 \vert X_1,X_2)$ is independent of $X_1$, and is the same function as $P(X_2 \vert X_1)$. Therefore
%
\begin{equation}\label{eq:tripvspair}
  \begin{aligned}
    H(X_3|X_1,X_2) &= H(X_2|X_1).\\
    I_3 &= 2I_2, \qquad C_3 = \frac{I_3}{2H_1} = C_2.
  \end{aligned}
\end{equation}
%
Similar arguments apply for longer $r$-tuples:
 %
 \begin{equation}\label{eq:markovinf}
   I_r = (r-1)I_2, \qquad C_r = \frac{I_r}{(r-1)H_1} = C_2.
\end{equation}
%
One can look for non-Markovian behaviour by looking for differences in $C_r$ for different $r$.


\subsection{Effect of animal-to-animal variability}\label{sec:variability}

In this section we will look at the effect that animal-to-animal variability can have on mutual information in the case of binary output, \ie the characteristics of each reorientation are reduced to two possible outcomes which we will call $\{+,-\}$. In specific cases these two outcomes will be the two most important reorientation types (omega or reversal-omega) and the reorientation direction (right or left).

Label the individual worms with $i=1\ldots N$. Then, for a single reorientation
%
\begin{equation}\label{eq:varisinglereo}
  P(i) = q_i, \qquad
  P(+|i) = p_i, \qquad
  P(-|i) = 1-p_i.
\end{equation}
%
The prior probability that a reorientation was from a particular animal, $q_i$, will be proportional to that animal's reorientation rate, which may not be the same for all animals. We also introduce the notation
%
\begin{equation}\label{eq:varinot}
  \bar{p} = \sum_i q_i p_i, \qquad
  \sigma_p = \sqrt{\sum_i q_i(p_i-\bar{p})^2}.
\end{equation}
%
So that
%
\begin{equation}\label{eq:varimarg}
  P(+) = \bar{p}, \qquad P(-) = 1-\bar{p}.
\end{equation}
%
Then, using Bayes' theorem
%
\begin{equation}\label{eq:singlereobayes}
  P(i|+) = \frac{P(+|i)P(i)}{P(+)} = \frac{q_ip_i}{\bar{p}}, \qquad
  P(i|-) = \frac{P(-|i)P(i)}{P(-)} = \frac{q_i(1-p_i)}{1-\bar{p}}.
\end{equation}
%

This allows us to calculate conditional probabilities for next reorientation, given previous reorientation
%
\begin{equation}\label{eq:varicond}
  \begin{aligned}
    P(+|+) &= \sum_i P(+|i)P(i|+) = \bar{p} + \frac{\sigma_p^2}{\bar{p}}, \\
    P(+|-) &= \sum_i P(+|i)P(i|-) = \bar{p} - \frac{\sigma_p^2}{1-\bar{p}}, \\
    P(-|+) &= \sum_i P(-|i)P(i|+) = 1-\bar{p} - \frac{\sigma_p^2}{\bar{p}}, \\
    P(-|-) &= \sum_i P(-|i)P(i|-) =1- \bar{p} + \frac{\sigma_p^2}{1-\bar{p}},
  \end{aligned}
\end{equation}
%
and the joint probabilities of previous and next reorientations
%
\begin{equation}\label{eq:varijoint}
  \begin{aligned}
    P(+,+) &= P(+|+)P(+) = \bar{p}^2 + \sigma_p^2, \\
    P(+,-) &= P(-|+)P(+) = \bar{p}(1-\bar{p}) - \sigma_p^2, \\
    P(-,+) &= P(+|-)P(-) = \bar{p}(1-\bar{p}) - \sigma_p^2, \\
    P(-,-) &= P(-|-)P(-) = (1-\bar{p})^2 + \sigma_p^2.
  \end{aligned}
\end{equation}
%

The entropy of single reorientations is then
%
\begin{equation}\label{eq:varient}
  H_1 = - \sum_{a\in\{+,-\}} P(a)\log P(a)
   = - \bar{p}\log \bar{p} - (1-\bar{p})\log(1-\bar{p}),
\end{equation}
%
and the mutual information is
%
\begin{equation}\label{eq:varimutinf}
  \begin{aligned}
  I_2 =& \sum_{a,b\in\{+,-\}} P(a,b)\log \frac{P(a,b)}{P(a)P(b)}\\
   =& \prn{\bar{p}^2 + \sigma_p^2}\log\prn{1 + \frac{\sigma_p^2}{\bar{p}^2}}
   + 2\prn{\bar{p}(1-\bar{p}) - \sigma_p^2}\log\prn{1 - \frac{\sigma_p^2}{\bar{p}(1-\bar{p})}}
   \\&+ \prn{(1-\bar{p})^2 + \sigma_p^2}\log\prn{1 + \frac{\sigma_p^2}{(1-\bar{p})^2}},
  \end{aligned}
\end{equation}
%
which vanishes when $\sigma_p=0$.


\section{Practical issues}\label{sec:practical}

\subsection{Correcting systematic errors}\label{sec:syscorr}

We will compute the quantities defined in the previous section by estimating the probability distributions, $P_1(X_i)$ and $P_r(X_1,\ldots,X_r)$ from the relative frequencies in sample sequences of reorientations. As all the $X_i$ have the same marginal distribution, we will estimate $P_1(X)$ from the pooled data, rather that estimating the $P_1(X_i)$ separately.


The lower bounds on mutual information, \eqref{eq:mutinfbounds} and \eqref{eq:normmutinf}, lead to systematic errors which tend to bias these estimates upwards. There are several methods for estimating this bias.

One approach involves expending the errors in the reciprocal of the sample size and estimating the leading order correction, see \cite{Roulston1999}. One can also estimate the random errors using this approach. Our estimators are slightly different to those used there, the appropriate versions of the estimates are calculated in Appendix \ref{sec:stderr}.

As the bias estimates are independent of the actual probability distribution, depending only on sample size and number of bins, we can estimate the bias by computing the mutual information for a completely random sequence in the same way. As the true value is zero, the result of this computation is an estimate of the bias. Furthermore, if the probabilities of the individual elements of the sequence are the same as in the data, this can be regarded as a Monte-Carlo simulation of the null hypothesis -- that the individual elements of the sequence are independent of each other. Thus, we can compute a p-value by seeing where the original result ranks amongst the results of the simulation.

We will do this using the non-parametric bootstrap. This is conceptually similar to the common alternative procedure of shuffling the sequence. Shuffling can be thought of as resampling without replacement, whereas the non-parametric bootstrap is resampling with replacement. They both involve removing any information in the sequence without changing the probabilities of the individual elements.

The direct method of \cite{Strong1998} consists of varying the sample size and extrapolating to infinity. This can also be used to check that the number of samples is large enough compared to the number of bins. With this method, it is difficult to compute error bars and p-values, as the process is slow.

We show three examples of these methods in \fref{fig:biascomp}, one where they agreed really well, one where the agreement was not too bad and one where it was terrible. In the last case, it even showed the wrong trend: the bias is supposed to be positive and decreasing with increasing sample size. This probably indicates that the sample size is too small and we were not in the asymptotic regime where the results of appendix \ref{sec:stderr} can be trusted.

We can see how much this discrepancy varied with sample size in \fref{fig:discrep}. This can be used as a guide for when $N$ is large enough to trust the results.


\begin{figure}
  \begin{center}
    (a)\includegraphics[width=7cm]{comparison.eps}
    (b)i)\includegraphics[width=7cm]{spat_dur_3.eps}\\
    ii)\includegraphics[width=7cm]{warm_typ_2.eps}
    iii)\includegraphics[width=7cm]{iso_ang_2.eps}
  \end{center}
  \caption{Comparison of different methods for removing bias. (a) Three examples of the three unbiased estimates of mutual information/total correlation. First-order refers to the methods of appendix \ref{sec:stderr}. Monte-Carlo refers to subtracting the mean of 1000 nonparametric bootstrap simulations, the error-bars are the standard deviation of the bootstrap simulations. (b) Illustration of the direct method, Blue line is the uncorrected estimates with different sample sizes, red dashed line shows the extrapolation to infinite sample size and red dotted line indicates the result of this extrapolation. Green line shows the unbiased estimator using the first-order method of appendix \ref{sec:stderr} for comparison, darker green shading is $\pm$ one standard error. i)-iii) the three cases shown in (a).} \label{fig:biascomp}
\end{figure}

\begin{figure}
  \begin{center}
    (a)\includegraphics[width=7cm]{discpair.eps}
    (b)\includegraphics[width=7cm]{disctrip.eps}
  \end{center}
  \caption{Effect of sample size in discrepancy between bias removal methods; size of discrepancy between unbiased estimates from direct method and first order method divided by first order standard error for (a) pairs (maximum of 25 bins) (b) triplets (maximum of 125 bins).} \label{fig:discrep}
\end{figure}


\subsection{Estimating animal-to-animal variability}\label{sec:variest}

Estimating $\sigma_p$ in \eqref{eq:varinot} is tricky, as we can't keep track of animal identity during collisions. What we will do is to restrict attention to those tracks that start during the first $20\mins$ and are at least $20\mins$ long. This ensures that each animal is sampled at most once and that we have a decent amount of data for each animal. We will then assume that the estimate for $\sigma_p$ applies to the whole experiment.

For reorientation type, we count the number of omegas in each track, $\Omega_i$, and the number of reversal-omegas, $R_i$. If the track length is $t_i$,
%
\begin{equation}\label{eq:varesttype}
  r_i = \frac{\Omega_i+R_i}{t_i}, \qquad
  q_i = \frac{r_i}{\sum_j r_j}, \qquad
  p_i = \frac{\Omega_i}{\Omega_i+R_i}.
\end{equation}
%

For reorientation direction, we count the number of right turns in each track, $R_i$, and the number of left turns, $L_i$. If the track length is $t_i$,
%
\begin{equation}\label{eq:varestdir}
  r_i = \frac{R_i+L_i}{t_i}, \qquad
  q_i = \frac{r_i}{\sum_j r_j}, \qquad
  p_i = \frac{R_i}{R_i+L_i}.
\end{equation}
%

In both cases, $\bar{p}$ and $\sigma_p$ can be calculated by substituting \eqref{eq:varesttype} or \eqref{eq:varestdir} into \eqref{eq:varinot}.

The estimators for $\sigma_p$ may be biased, but they should be consistent. As the sample sizes are large, the bias should be negligible. We will not attempt to calculate standard errors for these estimators.



\section{Results}\label{sec:results}

\subsection{Continuous cases}

We computed the mutual information in successive reorientation angles (the total change in heading direction during the reorientation)  and run durations (the time between this reorientations and the previous one). As these are continuous variables, the data has to be binned. We will follow the approach of \cite{Slonim:2005} and place the bins on quantiles of the data, preserving the coordinate invariance of the mutual information. In both cases, we will use 5 bins.

%The results of this analysis are presented in \fref{fig:results}. The analysis was performed for an isotropic assay (where the temperature was uniform and constant), a spatial assay (where the temperature was constant in time but varied linearly in space from $18-23\dC$ over $9\cm$) and a temporal assay (where the temperature was constant in space but varied sinusoidally in time from $19.7-21\dC$ with a period of $10\mins$). For the temporal assay, the analysis was also performed restricted to periods of warming ($\diff{T}{t} > 10^{-4}\mbox{}\dC/\mathrm{s}$) and cooling ($\diff{T}{t} < -10^{-4}\mbox{}\dC/\mathrm{s}$).

The results of this analysis are presented in \fref{fig:results}. The analysis was performed for isotropic assays (where the temperature was held uniform and constant at $20\dC$) and spatial assays (where the temperature was constant in time but varied linearly in space from $18-23\dC$ over $22\cm$). These were done with worms cultivated at $20\dC$ (neutral), $15\dC$ (cryophilic) ant $25\dC$ (thermophilic).

We looked at the cases $r=2$ (pairs of consecutive reorientations) and $r=3$ (triplets of consecutive reorientations). We also restricted attention to pairs of reorientations whose starts were separated by less than $30\s$ as well as pairs separated by more than $30\s$.

%Note that when we impose a restriction on the separation of reorientations, it is no longer true that the marginal distributions of the first and second run durations in a pair are identical. This was not dealt with properly, so the corresponding bars have been excluded in \fref{fig:results}. If run duration can affect reorientation type or angle, this same problem could exist in those cases. In \fref{fig:results2} we repeat the analysis using \eqref{eq:normmutinfgen} instead of \eqref{eq:normmutinf} for these cases only, in which case the uncertainty and bias can be calculated using the methods of \cite{Roulston1999} without modification.

Note that when we impose a restriction on the separation of reorientations, it is no longer true that the marginal distributions of the first and second run durations in a pair are identical. If run duration can affect reorientation type or angle, this same problem could exist in those cases. Therefore we performed the analysis using \eqref{eq:normmutinfgen} instead of \eqref{eq:normmutinf} for these cases only, in which case the uncertainty and bias can be calculated using the methods of \cite{Roulston1999} without modification.

%\begin{figure}
%  \begin{center}
%    \includegraphics[width=8cm]{isotropic.eps}
%    \includegraphics[width=8cm]{spatial.eps}\\
%    \includegraphics[width=8cm]{temporal.eps}
%    \includegraphics[width=8cm]{warming.eps}\\
%    \includegraphics[width=8cm]{cooling.eps}
%  \end{center}
%  \caption{Normalised total correlation/mutual information for (a) isotropic assay, (b) spatial assay, (c) temporal assay, (d) temporal assay restricted to times when $\diff{T}{t} > 10^{-4}\mbox{}\dC/\mathrm{s}$, (e) temporal assay restricted to times when $\diff{T}{t} < -10^{-4}\mbox{}\dC/\mathrm{s}$. Values have bias subtracted using the methods of appendix \ref{sec:stderr}. Error bars are one standard error, computed using the methods of appendix \ref{sec:stderr}. P-values computed using 1000 nonparametric bootstrap resamples under the null hypothesis that successive reorientations are independent, therefore the third decimal place is untrustworthy. $N$ is the number of $r$-tuples in the sample.} \label{fig:results}
%\end{figure}

%\begin{figure}
%  \begin{center}
%    (a)\includegraphics[width=7cm]{reotype.eps}
%    (b)\includegraphics[width=7cm]{reoangle.eps}\\[1cm]
%    (c)\includegraphics[width=7cm]{rundur.eps}
%  \end{center}
%  \caption{Normalised total correlation/mutual information in successive (a) reorientation types, (b) reorientation angles and (c) run durations for isotropic assay, spatial assay, temporal assay, temporal assay restricted to times when $\diff{T}{t} > 10^{-4}\mbox{}\dC/\mathrm{s}$ and temporal assay restricted to times when $\diff{T}{t} < -10^{-4}\mbox{}\dC/\mathrm{s}$. Values have bias subtracted using the methods of appendix \ref{sec:stderr}. Error bars are one standard error, computed using the methods of appendix \ref{sec:stderr}. P-values were computed using 1000 nonparametric bootstrap resamples under the null hypothesis that successive reorientations are independent, therefore the third decimal place is untrustworthy. $N$ is the number of $r$-tuples in the sample.} \label{fig:results}
%\end{figure}
%
%\begin{figure}
%  \begin{center}
%    (a)\includegraphics[width=7cm]{reotype2.eps}
%    (b)\includegraphics[width=7cm]{reoangle2.eps}\\[1cm]
%    (c)\includegraphics[width=7cm]{rundur2.eps}
%  \end{center}
%  \caption{Same as \fref{fig:results}, but we use \eqref{eq:normmutinfgen} instead of \eqref{eq:normmutinf} for the cases where we restrict attention to pairs of reorientations whose starts were separated by less than $20\s$ or pairs separated by more than $20\s$.} \label{fig:results2}
%\end{figure}

\begin{figure}
  \begin{center}
    (a)\includegraphics[width=7cm]{reoangle3.eps}
    (b)\includegraphics[width=7cm]{rundur3.eps}
  \end{center}
  \caption{Normalised total correlation/mutual information in successive (a) reorientation angles and (b) run durations for worms grown at $20\dC$ on no gradient at $20\dC$, worms grown at $15\dC$ on spatial gradient from $18-23\dC$ over $22\cm$ and worms grown at $25\dC$ on spatial gradient from $18-23\dC$ over $22\cm$. Values have bias subtracted using the methods of appendix \ref{sec:stderr}. Error bars are one standard error, computed using the methods of appendix \ref{sec:stderr}. P-values were computed using 1000 nonparametric bootstrap resamples under the null hypothesis that successive reorientations are independent, therefore the third decimal place is untrustworthy. $N$ is the number of $r$-tuples in the sample.} \label{fig:results}
\end{figure}

\subsection{Binary cases}\label{sec:binaryresults}

Now we will restrict attention to binary cases, \ie for reorientation type we only look at two types: omega and reversal-omega. For reorientation angle we only pay attention to the sign, \ie whether it is to the right or to the left.

When looking at reorientation types, we will restrict attention to the 2 most common types - \emph{omega} and \emph{reversal-omega} (the probabilities of the different types are shown in \fref{fig:typeprob}).

The analysis was performed for isotropic assays (where the temperature was held uniform and constant at $20\dC$) and spatial assays (where the temperature was constant in time but varied linearly in space from $18-23\dC$ over $22\cm$). These were done with worms cultivated at $20\dC$ (neutral), $15\dC$ (cryophilic) ant $25\dC$ (thermophilic).

We only looked at the cases $r=2$ (pairs of consecutive reorientations). The results are in \fref{fig:binaryresults}. 


\begin{figure}
  \begin{center}
    (a)\includegraphics[width=7cm]{ReoTypeProb-neutraliso.eps}\\[1cm]
    (b)\includegraphics[width=7cm]{ReoTypeProb-cryospat.eps}
    (c)\includegraphics[width=7cm]{ReoTypeProb-thermospat.eps}
  \end{center}
  %\input{.TpX}
  \caption{Probabilities of different reorientation types for (a) worms grown at $20\dC$ on no gradient at $20\dC$, (b) worms grown at $15\dC$ on spatial gradient from $18-23\dC$, (c)  worms grown at $25\dC$ on spatial gradient from $18-23\dC$.  Reorientation labels: w - Omega turn, r - reversal, s - unreversal. }\label{fig:typeprob}
\end{figure}


\begin{figure}
  \begin{center}
    (a)\includegraphics[width=7cm]{cond_type.eps}
    (b)\includegraphics[width=7cm]{cond_dir.eps}\\[1cm]
    (c)\includegraphics[width=7cm]{reotype_bin.eps}
    (d)\includegraphics[width=7cm]{reoangle_bin.eps}
  \end{center}
  \caption{Conditional probabilities and mutual information for worms grown at $20\dC$ on no gradient at $20\dC$, worms grown at $15\dC$ on spatial gradient from $18-23\dC$ over $22\cm$ and worms grown at $25\dC$ on spatial gradient from $18-23\dC$ over $22\cm$. (a-b) Conditional probabilities for (a) reorientation types and (b) reorientation directions. Data refers to the actual measured values, variability refers to the predictions assuming that the only effect is due to animal-to animal variability as discussed in \sref{sec:variability}, with the variability measured as discussed in \sref{sec:variest}. (c-d) Normalised total correlation/mutual information in successive (c) reorientation types and (d) reorientation angles.  Values have bias subtracted using the methods of appendix \ref{sec:stderr}. Error bars are one standard error, computed using the methods of appendix \ref{sec:stderr}. P-values were computed using 1000 nonparametric bootstrap resamples under the null hypothesis that successive reorientations are independent, therefore the third decimal place is untrustworthy. $N$ is the number of $r$-tuples in the sample.} \label{fig:binaryresults}
\end{figure}






\appendix\section*{Appendices}

\section{Bias and standard error}\label{sec:stderr}

We will follow the approach of \cite{Roulston1999}. Our situation is slightly different from that one. As all the $X_i$ have the same distribution, we will estimate $P(X)$ from the pooled data, rather that estimating the $P(X_i)$ separately. This means that our estimates may not satisfy the bounds, such as \eqref{eq:mutinfbounds}.

Let $p_{i_1\ldots i_r}$ denote the probability $P_r(X_1=x_{i_1},\ldots,X_r=x_{i_r})$ and $n_{i_1\ldots i_r}$ denote the number of corresponding r-tuples in the sample. We can estimate $p_{i_1\ldots i_r}$ with
%
\begin{equation}\label{eq:tupprob}
  q_{i_1\ldots i_r} = \frac{n_{i_1\ldots i_r}}{N},
  \qquad
  N = \sum_{j_1\ldots ji_r} n_{j_1\ldots j_r}.
\end{equation}
%
We can then estimate $p_{j}=P_1(X=x_{j})$ with
%
\begin{equation}\label{eq:singprob}
  q_j = \sum_{i_1\ldots i_r}\prn{ \frac{q_{i_1\ldots i_r}}{r} \sum_{a=1}^r \delta_{j,i_a}}.
\end{equation}
%

From now on, we will use $A$ to denote the estimate of $A(p)$ with $p$ replaced by $q$ and $\widehat{A}=A-\bias(A)$, where $A$ is one of $(H_1,H_r,I_r,C_r)$.

Our bias estimates are essentially the same as those of \cite{Roulston1999}, except that the number of samples for $H_1$ is $rN$ rather than $N$:
%
\begin{equation}\label{eq:biasH}
  B_1 = \bias(H_1) = -\frac{\#b_1}{2rN},
  \qquad
  B_r = \bias(H_r) = -\frac{\#b_r}{2N},
\end{equation}
%
where $\#b_r$ is the number of non-empty bins (\eg in \fref{fig:schematic}, $\#b_1=3$, $\#b_2=6$). The bias estimate for $I$ and $C$ follow in the usual way.

We can estimate the standard errors with
%
\begin{equation}\label{eq:stderr}
  \var(A) \approx \sum_{i_1\ldots i_r} \prn{\pdiff{A}{n_{i_1\ldots i_r}}}^2 \var(n_{i_1\ldots i_r}),
  \qquad
  \var(n_{i_1\ldots i_r}) \approx N q_{i_1\ldots i_r}(1-q_{i_1\ldots i_r}).
\end{equation}
%
where the first formula is valid provided each term is small(we'll see later that they are proportional to $1/N$) and the corrections to the last formula are lower order in $N$.

We find that
%
\begin{equation}\label{eq:dqbydn}
  \begin{aligned}
    \pdiff{q_{i_1\ldots i_r}}{n_{j_1\ldots j_r}} &= \frac{\prn{\prod_a \delta_{i_a,j_a}} - q_{i_1\ldots i_r}}{N}, &
    \qquad
    \pdiff{B_{r}}{n_{j_1\ldots j_r}} &= -\frac{B_r}{N},\\
    \pdiff{q_{i}}{n_{j_1\ldots j_r}} &= \frac{\frac{1}{r} \prn{\sum_a \delta_{i,j_a}} - q_{i}}{N}, &
    \pdiff{B_{1}}{n_{j_1\ldots j_r}} &= -\frac{B_1}{N},
   \end{aligned}
\end{equation}
%
which leads to
%
\begin{equation}\label{eq:dhbydn}
  \begin{aligned}
    \pdiff{H_r}{n_{j_1\ldots j_r}} &= -\frac{\log q_{j_1\ldots j_r} + H_r}{N}, &
    \qquad
    \pdiff{I_{r}}{n_{j_1\ldots j_r}} &= -\frac{\prn{\sum_a \log q_{j_a}}-\log q_{i_1\ldots i_r} + I_r}{N}, \\
    \pdiff{H_1}{n_{j_1\ldots j_r}} &= -\frac{\frac{1}{r} \prn{\sum_a \log q_{j_a}} + H_1}{N}, &
    \pdiff{C_{r}}{n_{j_1\ldots j_r}} &= \frac{\log q_{i_1\ldots i_r} H_1 - \frac{1}{r} \prn{\sum_a \log q_{j_a}} H_r}{(r-1)N(H_1)^2}.
   \end{aligned}
\end{equation}
%
All of these formulae are equally true if you put hats on every capital letter (except $N$).


%\section*{Acknowledgements}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section*{Appendices}
%\appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{utcaps_sl}
\bibliography{neuro,maths}

\end{document}
