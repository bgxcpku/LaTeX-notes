% -*- TeX -*- -*- UK -*-
% ----------------------------------------------------------------
% arXiv Paper ************************************************
%
% Subhaneil Lahiri's template
%
% Before submitting:
%    Comment out hyperref
%    Comment out showkeys
%    Replace \input{mydefs.tex} with its contents
%       or include mydefs.tex in zip/tar file
%    Replace \input{newsymb.tex} with its contents
%       or include newsymb.tex in zip/tar file
%    Put this file, the .bbl file, any picture or
%       other additional files and natbib.sty
%       file in a zip/tar file
%
% **** -----------------------------------------------------------
\documentclass[12pt]{article}
% Preamble:
\usepackage{a4wide}
\usepackage[centertags]{amsmath}
\usepackage{amssymb}
%\usepackage{amsthm}
\usepackage[sort&compress,numbers]{natbib}
%\usepackage{citeB}
\usepackage{ifpdf}
%\usepackage{graphicx}
%\usepackage{graphics} for finding documentation only
%\usepackage{xcolor}
%\usepackage{pgf}
\ifpdf
\usepackage[pdftex,bookmarks]{hyperref}
\else
\usepackage[hypertex]{hyperref}
\DeclareGraphicsRule{.png}{eps}{.bb}{}
\fi
%
% >> Only for drafts! <<
\usepackage[notref,notcite]{showkeys}
% ----------------------------------------------------------------
\vfuzz2pt % Don't report over-full v-boxes if over-edge is small
\hfuzz2pt % Don't report over-full h-boxes if over-edge is small
%\numberwithin{equation}{section}
%\renewcommand{\baselinestretch}{1.5}
% ----------------------------------------------------------------
% New commands etc.
\input{mydefs.tex}
\input{newsymb.tex}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title info:
\title{Mutual information between successive reorientations}
%
% Author List:
%
\author{Subhaneil Lahiri
\\
%
% Addresses:
%
\small{\emph{Harvard University,}}
%
}

\begin{document}

\maketitle



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{abstract}
  We show how mutual information can be used to describe the independence of successive reorientations
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beginning of Article:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Reorientation sequences}\label{sec:reoseq}

As a worm navigates, it performs a sequence of turns. When turns occur sufficiently close to each other, they are grouped into a reorientation event. These reorientations have several characteristics, \eg the types of turn of which it is composed, the difference in heading direction before and after, the duration of the run leading into it. We wish to know if the characteristics of one reorientation are independent of the characteristics of previous reorientations.

Consider a sequence of $r$ successive reorientations. The values of a particular characteristic of these reorientations is an $r$-tuple of random variables: $(X_1,\ldots,X_r)$. We are asking whether or not $P(X_1,\ldots,X_r) = P(X_1)\ldots P(X_r)$.

\section{Entropy and mutual information}\label{sec:entropy}

The \textbf{entropy} of a probability distribution is a measure of the lack of information we have about a random variable:
%
\begin{equation}\label{eq:ent}
  H(X) = \av{-\log P(X)}.
\end{equation}
%
It takes its minimum value of $0$ when $X$ can only take one value. It takes its maximum value of $\log n$ when $X$ has a uniform distribution over $n$ possibilities.

With several random variables, we can define a joint entropy from their joint probability distribution:
%
\begin{equation}\label{eq:jointent}
  H(X_1,\ldots,X_r) = \av{-\log P(X1,\ldots,X_r)}.
\end{equation}
%
It satisfies the bounds
%
\begin{equation}\label{eq:entbounds}
  \max_i H(X_i) \leq H(X_1,\ldots,X_r) \leq \sum_{i=1}^r H(X_i).
\end{equation}
%
The lower bound is saturated when one of the variables is enough to determine the others. The upper bound is saturated when the $X_i$ are independent:
%
\begin{equation}\label{eq:indent}
  P(X1,\ldots,X_r) = \prod_{i=1}^r P(X_i)
  \quad \implies \quad
  H(X_1,\ldots,X_r) = \sum_{i=1}^r H(X_i).
\end{equation}
%

We can define the following measure of (lack of) independence:
%
\begin{equation}\label{eq:mutinf}
  I(X_1,\ldots,X_r) = \sum_{i=1}^r H(X_i) - H(X_1,\ldots,X_r).
\end{equation}
%
In the case $r=2$, this is the \textbf{mutual information} between $X_1$ and $X_2$. For $r>2$, there are many different generalisations of mutual information. This one is called the \textbf{total correlation} \cite{Watanabe:1960:ITA:1661258.1661265}, or multiinformation. It has the properties:
%
\begin{itemize}
  \item it vanishes if and only if the random variables are independent
  \item otherwise, it is positive.
  \item it is bounded from above by $\sum_{i=1}^r H(X_i) - \max_i H(X_i)$.
\end{itemize}
%

In our cases, the random variables, $X_i$, all have the same distribution, so the total correlation satisfies the bounds
%
\begin{equation}\label{eq:mutinfbounds}
  0 \leq I_r(X_1,\ldots,X_r) \leq (r-1)H(X).
\end{equation}
%
We can define a normalised total correlation:
%
\begin{equation}\label{eq:normmutinf}
  C_r = \frac{I_r}{(r-1)H_1}, \qquad 0 \leq C_r \leq 1.
\end{equation}
%
The lower bound corresponds to complete independence. The upper bound corresponds to complete redundancy.


\appendix

\section{Bias and standard error}\label{sec:stderr}

We will follow the approach of \cite{1999PhyD..125..285R}. Our situation is slightly different from that one. As all the $X_i$ have the same distribution, we will estimate $P(X)$ from the pooled data, rather that estimating the $P(X_i)$ separately. This means that our estimates may not satisfy the bounds, such as \eqref{eq:mutinfbounds}.

Let $p_{i_1\ldots i_r}$ denote the probability $P(X_1=x_{i_1},\ldots,X_r=x_{i_r})$ and $n_{i_1\ldots i_r}$ denote the number of corresponding r-tuples in the sample. We can estimate $p_{i_1\ldots i_r}$ with
%
\begin{equation}\label{eq:tupprob}
  q_{i_1\ldots i_r} = \frac{n_{i_1\ldots i_r}}{N},
  \qquad
  N = \sum_{j_1\ldots ji_r} n_{j_1\ldots j_r}.
\end{equation}
%
We can then estimate $p_{j}=P(X=x_{j})$ with
%
\begin{equation}\label{eq:singprob}
  q_j = \sum_{i_1\ldots i_r}\prn{ \frac{q_{i_1\ldots i_r}}{r} \sum_{a=1}^r \delta_{j,i_a}}.
\end{equation}
%

From now on, we will use $A$ to denote the estimate of $A(p)$ with $p$ replaced by $q$ and $\widehat{A}=A-\bias(A)$.

Our bias estimates are essentially the same as those of \cite{1999PhyD..125..285R}, except that the number of samples for $H_1$ is $rN$ instead of $N$:
%
\begin{equation}\label{eq:biasH}
  B_1 = \bias(H_1) = -\frac{\#b_1}{2rN},
  \qquad
  B_r = \bias(H_r) = -\frac{\#b_r}{2N},
\end{equation}
%
where $\#b_r$ is the number of non-empty bins. The bias estimate for $I$ and $C$ follow in the same way.

We can estimate the standard errors with
%
\begin{equation}\label{eq:stderr}
  \var(A) = \sum_{i_1\ldots i_r} \prn{\pdiff{A}{n_{i_1\ldots i_r}}}^2 \var(n_{i_1\ldots i_r}),
  \qquad
  \var(n_{i_1\ldots i_r}) \approx N q_{i_1\ldots i_r}(1-q_{i_1\ldots i_r}).
\end{equation}
%
where the corrections to the last formula are lower order in $N$.

We find that
%
\begin{equation}\label{eq:dqbydn}
  \begin{aligned}
    \pdiff{q_{i_1\ldots i_r}}{n_{j_1\ldots j_r}} &= \frac{\prn{\prod_a \delta_{i_a,j_a}} - q_{i_1\ldots i_r}}{N}, &
    \qquad
    \pdiff{B_{r}}{n_{j_1\ldots j_r}} &= -\frac{B_r}{N},\\
    \pdiff{q_{i}}{n_{j_1\ldots j_r}} &= \frac{\frac{1}{r} \prn{\sum_a \delta_{i,j_a}} - q_{i}}{N}, &
    \pdiff{B_{1}}{n_{j_1\ldots j_r}} &= -\frac{B_1}{N}, 
   \end{aligned}
\end{equation}
%
which leads to
%
\begin{equation}\label{eq:dhbydn}
  \begin{aligned}
    \pdiff{H_r}{n_{j_1\ldots j_r}} &= -\frac{\log q_{i_1\ldots i_r} + H_r}{N}, &
    \qquad
    \pdiff{I_{r}}{n_{j_1\ldots j_r}} &= -\frac{\prn{\sum_a \log q_{j_a}}-\log q_{i_1\ldots i_r} + I_r}{N}, \\
    \pdiff{H_1}{n_{j_1\ldots j_r}} &= -\frac{\frac{1}{r} \prn{\sum_a \log q_{j_a}} + H_1}{N}, &
    \pdiff{C_{r}}{n_{j_1\ldots j_r}} &= \frac{\log q_{i_1\ldots i_r} H_1 - \frac{1}{r} \prn{\sum_a \log q_{j_a}} H_r}{(r-1)N(H_1)^2},
   \end{aligned}
\end{equation}
%
all of these formulae apply if you put hats on every capital letter.


%\section*{Acknowledgements}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section*{Appendices}
%\appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{utcaps_sl}
\bibliography{neuro,maths}

\end{document}
