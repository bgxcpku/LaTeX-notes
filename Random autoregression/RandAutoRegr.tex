% -*- TeX -*- -*- UK -*-
% ----------------------------------------------------------------
% arXiv Paper ************************************************
%
% Subhaneil Lahiri's template
%
% Before submitting:
%    Comment out hyperref
%    Comment out showkeys
%    Replace \input{mydefs.tex} with its contents
%       or include mydefs.tex in zip/tar file
%    Replace \input{newsymb.tex} with its contents
%       or include newsymb.tex in zip/tar file
%    Put this file, the .bbl file, any picture or
%       other additional files and natbib.sty
%       file in a zip/tar file
%
% **** -----------------------------------------------------------
\documentclass[12pt]{article}
%
% Preamble:
\input{sl_preamble.tex}
\input{sl_graphics_preamble.tex}
\graphicspath{{figs/}}
%
% >> Only for drafts! <<
\usepackage[notref,notcite]{showkeys}
%
% ----------------------------------------------------------------
%\numberwithin{equation}{section}
%\renewcommand{\baselinestretch}{1.5}
% ----------------------------------------------------------------
% New commands etc.
\input{sl_definitions.tex}
\input{sl_symbols.tex}
%matrices
\newcommand{\inv}{^{-1}}
\newcommand{\dg}{^\dagger}
\newcommand{\trans}{^\mathrm{T}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\omb}{\overline{\omega}}
\newcommand{\dw}{\dr w}
\newcommand{\dwb}{\dr\overline{w}}
\newcommand{\du}{\dr u}
\newcommand{\dub}{\dr\bar{u}}
\newcommand{\dv}{\dr v}
\newcommand{\dvb}{\dr\bar{v}}
% ----------------------------------------------------------------
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title info:
\title{Illusions of criticality in high-dimensional autoregressive models}
%
% Author List:
%
\author{Subhaneil Lahiri and Surya Ganguli
%
}

\begin{document}

\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{abstract}
  We look at the eigenvalue spectrum of high-dimensional autoregressive models when applied to white-noise.
\end{abstract}

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beginning of Article:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The problem}\label{sec:theprob}

Consider a model of the following type
%
\begin{equation}\label{eq:model}
  x(t+1) = A x(t) + \text{noise},
\end{equation}
%
where $x(t)$ is an $N$-element vector and $A$ is an $N\times N$ matrix.

Suppose we have a sample of $P$ consecutive times, so $x$ is an $N\times P$ matrix.
We can perform a least-squares estimate of $A$ by minimising the quantity
%
\begin{equation}\label{eq:minL}
  \half\sum_{i,\mu} \prn{ x_{i\mu+1} - \sum_j A_{ij} x_{j\mu} }^2 = \half\Tr \prn{xU-Ax}\prn{xU-Ax}\trans,
\end{equation}
%
where $U$ is a shift matrix.
It will be useful to use periodic boundary conditions in time, \ie $x_{iP+1}\sim x_{i1}$,
as this will make $U$ orthogonal:
%
\begin{equation}\label{eq:Udef}
  U_{\mu\nu} = \delta_{\mu\nu+1} + \delta_{\mu1}\delta_{\nu P}.
\end{equation}
%
The estimate of $A$ is then
%
\begin{equation}\label{eq:Aest}
  A = \prn{xUx\trans}\prn{xx\trans}\inv.
\end{equation}
%

Suppose we attempted this analysis in a situation where there really is no structure,
\ie when $x(t)$ is white noise.
Then the true optimal $A$ would be 0.
However, with finite $P$ the estimate \eqref{eq:Aest} will not be zero.

We will look at the average eigenvalue distribution:
%
\begin{equation}\label{eq:eigdist}
  \rho(\omega) = \av{\rho_A(\omega)}_x,
  \qquad
  \rho_A = \sum_{i=1}^N \delta(\omega-\lambda_i),
\end{equation}
%
where $\lambda_i$ are the eigenvalues of $A$ in \eqref{eq:Aest} and
the components of $x$ are iid gaussian random variables with mean 0 and variance 1.

Following \cite{Sommers1988asymmetric}, this can be computed from a potential:
%
\begin{equation}\label{eq:potential}
  \rho_A(\omega) = -\nabla^2 \Phi_A(\omega),
  \qquad
  \Phi_A(\omega) = -\frac{1}{4\pi N} \ln\det \brk{(\omb-A\trans)(\omega-A)}.
\end{equation}
%
We define a partition function
%
\begin{equation}\label{eq:partfn}
  \Phi_A(\omega) = \frac{1}{4\pi N} \ln Z_A(\omega),
  \qquad
  Z_A(\omega) = \det \brk{(\omb-A\trans)(\omega-A)}\inv.
\end{equation}
%
The problem is now to compute $\av{\ln Z_A(\omega)}_x$.



\section{Simplified derivation}\label{sec:simplederiv}

In this section, we will present a simplified version of the derivation.
We will make two simplifying assumptions.

First, we will treat $x$ as annealed, rather than quenched, disorder:
%
\begin{equation}\label{eq:annealed}
  \av{\ln Z_A(\omega)}_x = \ln\av{Z_A(\omega)}_x.
\end{equation}
%
We will justify this assumption in \sref{sec:replicader} using the replica trick.
We will see that, with a replica symmetric ansatz, the saddle point has zero off-diagonal replica overlaps.
This means that there is no coupling between the replicas, which produces identical results to the annealed calculation.

Second, we will assume factorisation of an expectation value:
%
\begin{equation}\label{eq:factorass}
  \av{\det(xx\trans)^2\cdots}_x = \av{\det(xx\trans)^2}_x \av{\cdots}_x .
\end{equation}
%
This will also be justified, in the large $N$ limit, in \sref{sec:replicader}.

We start with the representation of the determinant in \eqref{eq:compgausint}.
However, the matrix in \eqref{eq:partfn} is not positive-definite when $\omb$ is equal to one of the eigenvalues.
We can fix this by adding $\epsilon^2I$ and letting $\epsilon\to0$ at the end.
%
\begin{equation}\label{eq:partfnintz}
\begin{aligned}
  Z_A(\omega) &= \int \prod_i\frac{\dz_i\dzb_i}{2\pi} \exp\prn{
    -z\dg (\omb-A\trans)(\omega-A) z - \epsilon^2 z\dg z} .%\\
%    &=  \int \prod_i\frac{\dz_i\dzb_i}{2\pi} \exp\prn{
%    -z\dg (xx\trans)\inv x(\omb-U)x\trans x(\omega-U)x\trans (xx\trans)\inv z
%    - \epsilon^2 z\dg z}. \\
\end{aligned}
\end{equation}
%
Looking at the expression \eqref{eq:Aest} for $A$, we make the change of variables $z=(xx\trans)w/P$.
%
\begin{equation}\label{eq:partfnintw}
\begin{aligned}
  Z_A(\omega) =&\, \det(xx\trans)^2 \int \prod_i\frac{\dw_i\dwb_i}{2\pi} 
    \\&\quad\exp\prn{
    -\frac{1}{P^2} w\dg x(\omb-U)x\trans x(\omega-U)x\trans w - \frac{\epsilon^2}{P^2} w\dg xx\trans xx\trans w} .%\\
\end{aligned}
\end{equation}
%
We now take advantage of \eqref{eq:compgausslin} by introducing two standard complex Gaussian random vectors ($C=I$ in \eqref{eq:compgaussnorm}), $u$ and $v$:
%
\begin{equation}\label{eq:partfnintwuv}
\begin{aligned}
  Z_A(\omega) =&\, \det(xx\trans)^2 \int \prod_i\frac{\dw_i\dwb_i}{2\pi} \av{\e^{\ir S}}_{u,v},\\
  S =&\,
    \frac{ w\dg x(\omb-U)x\trans u + u\dg x(\omega-U)x\trans w + \epsilon w\dg xx\trans v + \epsilon v\dg xx\trans w}{P} .%\\
\end{aligned}
\end{equation}
%


\section{Full, replica-tastic derivation}\label{sec:replicader}



%\subsection*{Acknowledgements}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Appendices}
\appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Complex Gaussian integrals}\label{sec:compgauss}

First, Let's get all of the factors of 2 straight.
Note that if we write $z=x+\ir y$, then $\dz\dzb = 2\dx\dy$.
Let $H$ be a positive-definite, $N\times N$ Hermitian matrix.
Consider an integral  of the form
%
\begin{equation*}
  \int \prn{\prod_i\dz_i\dzb_i} \exp\prn{-z\dg H z}.
\end{equation*}
%
We can diagonalise $H$ with a unitary change of variables:
%
\begin{equation}\label{eq:compgausint}
\begin{aligned}
  \int \prn{\prod_i\dz_i\dzb_i} \exp\prn{-z\dg H z} &=
    \prod_i \int \dz_i\dzb_i\, \exp\prn{-\lambda_i \abs{z_i}^2} \\
    &= \prod_i \int \dx_i\dy_i\, 2\exp\prn{-\lambda_i \prn{x_i^2+y_i^2}} \\
    &= \prod_i \frac{2\pi}{\lambda_i}\\
    &= \frac{(2\pi)^N}{\det H}.
\end{aligned}
\end{equation}
%

The proper normalisation for a Gaussian distribution is
%
\begin{equation}\label{eq:compgaussnorm}
  P(z,z\dg)\dz\dz\dg = \prn{\prod_i\frac{\dz_i\dzb_i}{2\pi}} \frac{\exp\prn{-z\dg C\inv z}}{\det C}.
\end{equation}
%
By completing the square, we can see that
%
\begin{equation}\label{eq:compgausslin}
  \av{\exp\prn{\zeta\dg z + z\dg\zeta}} = \exp\prn{\zeta\dg C\zeta}.
\end{equation}
%
Taking partial derivatives \wrt $\zeta_i$ and $\bar{\zeta}_i$, we find
%
\begin{equation}\label{eq:compgauscov}
  \av{zz\dg} = C.
\end{equation}
%

Now consider an integral of the form
%
\begin{equation}\label{eq:compgaussquad}
\begin{aligned}
  \av{\exp\prn{-z\dg A z}} &=
    \int \prn{\prod_i\frac{\dz_i\dzb_i}{2\pi}}\frac{\exp\prn{-z\dg (C\inv+A) z}}{\det C} \\
    &= \prn{\det C\det\prn{C\inv+A}}\inv \\
    &= \det\prn{I+CA}\inv.
\end{aligned}
\end{equation}
%


\section{Contour integrals for determinants}\label{sec:contourints}

In evaluating determinants, we will come across contour integrals of the form
%
\begin{equation}\label{eq:contourint}
  I(z) = \frac{1}{2\pi\ir}\int \frac{\dr\zeta}{\zeta} \ln(z-\zeta),
\end{equation}
%
where the contour is the unit circle in a counter-clockwise direction.
The contour might not be closed because of the branch cut.
We choose the branch of the logarithm so that
%
\begin{equation}\label{eq:branch}
  \arg\prn{\frac{\zeta-z}{z}} \in [0,2\pi],
\end{equation}
%
and we define $\theta=\arg z$. The branch cut is shown in \fref{fig:contours}.

\begin{figure}
 \begin{center}
 \begin{myenuma}
  \item\aligntop{\includegraphics[width=5cm]{contout.svg}}\label{fig:contout}
  \hspace{1cm}
  \item\aligntop{\includegraphics[width=5cm]{contin.svg}}\label{fig:contin}
 \end{myenuma}
 \end{center}
  \caption{Contours used to evaluate \eqref{eq:contourint}, (\ref{fig:contout}) when $\abs{z}>1$, (\ref{fig:contin}) when $\abs{z}<1$.}\label{fig:contours}
\end{figure}

If $\abs{z}>1$, we can use the contour $C_1$ in \fref{fig:contours}(\ref{fig:contout}).
Using the residue theorem:
%
\begin{equation}\label{eq:intout}
  I(z) = \frac{1}{2\pi\ir}\int_{C_1} \frac{\dr\zeta}{\zeta} \ln(z-\zeta) = \ln z.
\end{equation}
%

If $\abs{z}>1$, we can use the contour $C_2$ in \fref{fig:contours}(\ref{fig:contin}):
%
\begin{equation}\label{eq:contout}
C_2: \quad
  \begin{aligned}
    \zeta &= \e^{\ir\phi},                             & \phi &\in [\theta+\delta,\theta+2\pi-\delta], \\
    \zeta &= \e^{\ir(\theta+2\pi-\delta)}+xz\prn{1-\e^{\ir(2\pi-\delta)}},
                                                       & x    &\in [0,1], \\
    \zeta &= z - x \e^{\ir(\theta+2\pi-\delta)}, \quad & x    &\in [\abs{z}-1,-\epsilon], \\
    \zeta &= z + \epsilon\e^{-\ir\phi},                & \phi &\in [-\theta-2\pi+\delta,-\theta-\delta], \\
    \zeta &= z + x \e^{\ir(\theta+\delta)},          & x    &\in [\epsilon,1-\abs{z}.] \\
    \zeta &= \e^{\ir(\theta+\delta)}+(1-x)z\prn{1-\e^{\ir\delta}},
                                                       & x    &\in [0,1], \\
  \end{aligned}
\end{equation}
%
Using the residue theorem:
%
\begin{equation}\label{eq:intin}
  \frac{1}{2\pi\ir}\int_{C_2} \frac{\dr\zeta}{\zeta} \ln(z-\zeta) = \ln z.
\end{equation}
%
If we let $\delta,\epsilon\to0$, the second, fourth and sixth parts of the contour integral vanish, and the first part gives $I(z)$ in \eqref{eq:contourint}.
We're left with
%
\begin{equation}\label{eq:intinlim}
  \begin{aligned}
    \ln z =&\, I(z)
           -\frac{1}{2\pi\ir} \int_{\abs{z}-1}^{0} \frac{\e^{\ir\theta}\dx}{z-x\e^{\ir\theta}} \ln\prn{x\e^{\ir(\theta+2\pi)}} %\\&
           +\frac{1}{2\pi\ir} \int_{0}^{1-\abs{z}} \frac{\e^{\ir\theta}\dx}{z+x\e^{\ir\theta}} \ln\prn{-x\e^{\ir\theta}}  \\
      =&\, I(z)
           -\frac{1}{2\pi\ir} \int_{0}^{1-\abs{z}} \frac{\dx}{\abs{z}+x} \ln\prn{-x\e^{\ir(\theta+2\pi)}} %\\&
           +\frac{1}{2\pi\ir} \int_{0}^{1-\abs{z}} \frac{\dx}{\abs{z}+x} \ln\prn{-x\e^{\ir\theta}}  \\
      =&\, I(z) - \int_{0}^{1-\abs{z}} \frac{\dx}{\abs{z}+x}  \\
      =&\, I(z) + \ln\abs{z}.
  \end{aligned}
\end{equation}
%

Therefore:
%
\begin{equation}\label{eq:countourintresult}
  I(z) = \frac{1}{2\pi\ir}\int \frac{\dr\zeta}{\zeta} \ln(z-\zeta) 
   = \ln z - [\ln\abs{z}]_-
   = \ir \arg z + [\ln\abs{z}]_+,
\end{equation}
%
where $[x]_\pm = x \theta(\pm x)$ and $\theta(x)$ is the Heaviside step function.


\section{The quadratic function \texorpdfstring{$\Gamma(\zeta)$}{Gamma(zeta)}}\label{sec:Gamma}

In evaluating determinants in \sref{sec:simplederiv} and \sref{sec:replicader}, we come across the function
%
\begin{equation}\label{eq:Gammadef}
  \Gamma(\zeta) = (\alpha^2+\epsilon^2rt)\zeta + rs(\omb\zeta-1)(\omega-\zeta) = - rs\omb (\zeta-\zeta_+) (\zeta-\zeta_-).
\end{equation}
%
We will collect some useful features of $\zeta_\pm$ here.

First, by comparing the two forms of $\Gamma(\zeta)$, we see that:
%
\begin{align}
\label{eq:zpzm}
  \zeta_+ \zeta_- &= \frac{\omega}{\omb},\\
  \label{eq:zppzm}
  \zeta_+ + \zeta_- &= \frac{\alpha^2+\epsilon^2rt + rs(1+\abs{\omega})^2}{rs\omb},\\
  \label{eq:Gprime}
  \Gamma'(\zeta_\pm) = \mp rs\omb(\zeta_+ - \zeta_-),
\end{align}
%
and \eqref{eq:zpzm} tells us that $\abs{\zeta_+} \abs{\zeta_-}=1$.
Solving the equation $\Gamma(\zeta_\pm)=0$ gives
%
\begin{align}
\label{eq:zetapm}
  \zeta_\pm &= \frac{ \alpha^2+\epsilon^2rt + rs(1+\abs{\omega})^2
     \pm \sqrt{\brk{\alpha^2+\epsilon^2rt + rs(1+\abs{\omega})^2}
       - 4(rs)^2\abs{\omega}^2} }{2rs\omb},\\
\label{eq:zpmzm}
  \zeta_+ - \zeta_- &= \frac{\sqrt{\brk{\alpha^2+\epsilon^2rt + rs(1+\abs{\omega}^2)}
       - 4(rs)^2\abs{\omega}^2} }{rs\omb},\\
\label{eq:Gprimesol}
  \Gamma'(\zeta_\pm) &= \mp\sqrt{\brk{\alpha^2+\epsilon^2rt + rs(1+\abs{\omega})^2}
       - 4(rs)^2\abs{\omega}^2}.
\end{align}
%
Differentiating the equation $\Gamma(\zeta_\pm)=0$ gives
%
\begin{align}
\label{eq:dzpmdr}
  \pdiff{\zeta_\pm}{r} &=
    -\frac{\epsilon^2t\zeta_\pm + s(\omb\zeta_\pm-1)(\omega-\zeta_\pm)}{\Gamma'(\zeta_\pm)}
    &&= \frac{\alpha^2\zeta_\pm}{r\Gamma'(\zeta_\pm)},\\
\label{eq:dzpmds}
  \pdiff{\zeta_\pm}{s} &=
    -\frac{r(\omb\zeta_\pm-1)(\omega-\zeta_\pm)}{\Gamma'(\zeta_\pm)}
    &&= \frac{(\alpha^2+\epsilon^2rt)\zeta_\pm}{s\Gamma'(\zeta_\pm)},\\
\label{eq:dzpmdt}
  \pdiff{\zeta_\pm}{r} &=
    -\frac{\epsilon^2r\zeta_\pm}{\Gamma'(\zeta_\pm)}.
\end{align}
%

It will also de helpful to note that
%
\begin{equation}\label{eqdabsz}
  \frac{\dr\abs{\zeta_\pm}}{\abs{\zeta_\pm}} = \Re\prn{\frac{\dr\zeta_\pm}{\zeta_\pm}}.
\end{equation}
%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{utcaps_sl}
\bibliography{maths,qft,neuro}

\end{document}
